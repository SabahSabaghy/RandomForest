{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Random forest classification model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data, including ground truth data and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GDAL, NumPy, and matplotlib\n",
    "from osgeo import gdal, gdal_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Tell GDAL to throw Python exceptions, and register all drivers\n",
    "gdal.UseExceptions()\n",
    "gdal.AllRegister()\n",
    "\n",
    "# Read in the image and ground data recalled as roi\n",
    "img_ds = gdal.Open(r\"path\", gdal.GA_ReadOnly)\n",
    "roi_ds = gdal.Open(r\"path\", gdal.GA_ReadOnly)\n",
    "\n",
    "# gdal_array to read raster data as numeric array from file\n",
    "img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),\n",
    "              gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType)) \n",
    "\n",
    "for b in range (img.shape[2]):\n",
    "    img[:, :, b] = img_ds.GetRasterBand(b+1).ReadAsArray()\n",
    "\n",
    "roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.int16)\n",
    "\n",
    "\n",
    "# Display one band of image and ground truth (roi) data\n",
    "plt.subplot(121)\n",
    "plt.imshow(img[:, :, -2], cmap=plt.cm.Greys_r, vmin=-2000, vmax=10000) # vmin and vmax is defined based on your data\n",
    "plt.title('Input Data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Ground Truth Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_ds is not None: \n",
    "    print (\"band count: \" + str(img_ds.RasterCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Skicit-learn to split ground truth and image data into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Find how many valid entries are in the roi data -- i.e. how many ground truth data samples? in this data zero entries refer\n",
    "# to non-valid enteries. \n",
    "n_samples = (roi > 0).sum()\n",
    "print('There are {n} samples'.format(n=n_samples))\n",
    "\n",
    "# What are the classification targets? i.e. classvalue 1 refers to irrigated areas and classvalue 627 refers to irrigated pixels \n",
    "targets = np.unique(roi[roi > 0])\n",
    "print('The training data include {n} classes: {classes}'.format(n=targets.size, \n",
    "                                                                classes=targets))\n",
    "\n",
    "# features : used to make predictions (e.g. the image inputs (img_ds))\n",
    "# labels :  to be predicted (e.g. roi ground truth data)\n",
    "features = img[roi > 0, :]  # include all bands\n",
    "labels = roi[roi > 0]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# train_test_split: Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "# Make sure the split of data is correct\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the split of data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize our model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# select hyperparameters\n",
    "n_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "max_depth = [5, 8, 15, 25, 30, None]\n",
    "min_samples_split = [1, 2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10, 15]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "bootstrap = [True, False]\n",
    "oob_score = [True, False]\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf,\n",
    "             max_features = max_features, bootstrap = bootstrap, oob_score = oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding best machine learning model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When having limitted resources: conduct a Randomized Search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "randomF = RandomizedSearchCV(rf, hyperF, n_iter=10, n_jobs=-1, cv=3, verbose=1, random_state=42)\n",
    "\n",
    "# Train Random Forest Model\n",
    "bestF = randomF.fit(train_features, train_labels)\n",
    "print(\"The mean accuracy of the model is:\", bestF.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best estimator that was chosen by the search\n",
    "bestF.best_estimator_\n",
    "\n",
    "# Parameter setting that gave the best results on the hold out data.\n",
    "bestF.best_params_\n",
    "\n",
    "# check the cv results\n",
    "import pandas as pd\n",
    "pd.DataFrame(bestF.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    # Determine Performance Metrics \n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    # Get the Confusion Matrix, Classification report and ACcuracy\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "    print('Confusion Matrix:', confusion_matrix(test_labels, predictions))\n",
    "    print (classification_report(test_labels, predictions))\n",
    "    print(\"Accuracy:\", accuracy_score(test_labels, predictions) * 100) \n",
    "    \n",
    "    return predictions\n",
    "\n",
    "evalu = evaluate(bestF, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize confuse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "labels = {'class1': 'name1', 'class2': 'name2', 'classn':'namen'}\n",
    "label_temp = [class1, class2, class3, classn] # put value of classes here\n",
    "\n",
    "\n",
    "labels_list = [labels[key] for key in labels]\n",
    "\n",
    "cm = confusion_matrix(test_labels, evalu, label_temp)\n",
    "print(cm)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix \\n', fontweight='bold', fontsize = 16)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + labels_list, fontsize = 12)\n",
    "ax.set_yticklabels([''] + labels_list, fontsize = 12)\n",
    "\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, '{:0.0f}'.format(z), ha='center', va='center', color='white', fontweight='bold', fontsize = 12)\n",
    "\n",
    "plt.xlabel('\\n \\n True', fontweight='bold', fontsize = 12)\n",
    "plt.ylabel('Predicted', fontweight='bold', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an idea of which spectral bands are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = list(range(9))\n",
    "\n",
    "for b, imp in zip(bands, bestF.best_estimator_.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b = b, imp = imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the rest of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our full image, and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "new_shape = (img.shape[0] * img.shape[1], img.shape[2])\n",
    "\n",
    "img_as_array = img[:, :, :].reshape(new_shape)\n",
    "print('Reshaped from {o} to {n}'.format(o=img.shape,\n",
    "                                        n=img_as_array.shape))\n",
    "\n",
    "# Now predict for each pixel\n",
    "class_prediction = bestF.predict(img_as_array)\n",
    "\n",
    "# Reshape our classification map\n",
    "class_prediction = class_prediction.reshape(img[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the prediction as a raster file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raster (path, band_count, bands, srs, geoTransform, format='GTiff', dtype = gdal.GDT_Float32):\n",
    "    rows,cols = bands.shape\n",
    "    # Initialize driver & create file\n",
    "    driver = gdal.GetDriverByName(format)\n",
    "    dataset_out = driver.Create(path, cols, rows, 1, gdal.GDT_Float32)\n",
    "    dataset_out.SetGeoTransform(geoTransform)\n",
    "    dataset_out.SetProjection(srs)\n",
    "    # Write file to disk\n",
    "    dataset_out.GetRasterBand(1).WriteArray(bands)\n",
    "    dataset_out = None\n",
    "    \n",
    "# Get the geo-reference information\n",
    "geoTransform = img_ds.GetGeoTransform()\n",
    "srs = img_ds.GetProjection()\n",
    "bands = class_prediction\n",
    "path = r\"path\" # for instance you can save output in .tif format\n",
    "\n",
    "save_raster(path, 1, bands, srs, geoTransform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
